{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import requests\n",
    "import re # regex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *As of 4/16/2023, Spring 2023 CS61A URL is: [https://cs61a.org/academic-interns/](\"https://cs61a.org/academic-interns/\"). In future semeters however, it will most likely be changed to: [https://inst.eecs.berkeley.edu/~cs61a/sp23/academic-interns/](https://inst.eecs.berkeley.edu/~cs61a/sp23/academic-interns/). This project proceeds with the former, but for anyone who wants to duplicate these results (or for future me to look back on), do note the change because the current code will scrape the \"current\" semester of 61A, assuming they still have AIs.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most web scraping ventures, I start by seeing if I could get the contents of the `html` file first. I begin my intial dive with [UC Berkeley's Spring 2023 Semester's CS61A's Academic Interns (AIs)](https://cs61a.org/academic-interns/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta name=\"description\" content =\"CS 61A: Structure \n"
     ]
    }
   ],
   "source": [
    "# Scrape the html contents\n",
    "sp23_url = \"https://cs61a.org/academic-interns/\"\n",
    "# API call\n",
    "sp23_response = requests.get(sp23_url)\n",
    "# Store response into a string\n",
    "sp23_response_string = sp23_response.text\n",
    "\n",
    "print(sp23_response_string[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, unlike [my other web scraping project](https://github.com/dnhuang/sf-tri-city-restaurants/blob/main/scraper.ipynb), there doesn't seem to to be a JSON response file shown by Google Chrome developer tools. This means that I needed to do this the old fashioned way: regular expressions.\n",
    "\n",
    "Thankfully, online tools like [Regex101](https://regex101.com/) exists, which made finding a pattern much easier. But in the process of figuring out patterns, I noticed a lot of inconsistencies and edge cases that were very difficult to capture; all from **ONE** semester of 61A. Here are a few of them:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Names**\n",
    "\n",
    "I was an AI for 61A in Fall 2021, and I only recall inputting my first and last name, so I thought the pattern would be pretty simple. It turns out some people also listed their middle names, which meant there were at least two spaces. But then came the Indian names, which maybe had like five different words strung together. After accounting for these, I was still was getting the correct amount of matches (there are a total of $130$ AIs for Spring 2023 and yes, I manually counted them). So that's when I discovered that some people had hyphens (-) in their name. After adding that into the pattern, I was still left disappointed; $129/130$ matches. And that's when I saw this:\n",
    "\n",
    "<div>\n",
    "<img src='img/parentheses_in_name.png' width=500>\n",
    "</div>\n",
    "\n",
    "Of course, ***nicknames***."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pronouns**\n",
    "\n",
    "Pronouns weren't as bad as names, but that might be because I didn't bother spending time to make the pattern look concise; I'm kind of just over it. An edge case I ran into was this:\n",
    "\n",
    "<div>\n",
    "<img src='img/multi_pronoun_sp23.png' width=500>\n",
    "<img src='img/multi_pronoun_html_sp23.png' width=500>\n",
    "</div>\n",
    "\n",
    "And this was the reason I was getting $131$ matches instead of $130$. ***Multi-set pronouns***. WHY.\n",
    "\n",
    "I decided that I honestly couldn't be bothered to determine how to settle this issue, so I settled on just choosing the first set,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bio**\n",
    "\n",
    "This was by far the worst one to extract. In the beginning, I thought I solved it by just simply using `(.*)` as I ended up $124$ matches (missing $6%) so I just needed to tweak it a little bit further.\n",
    "\n",
    "But then I started to really struggle because I couldn't for the life of me figure out how to use `.*` within a `[]`, as I wanted to capture everything while account for `\\s*`, which needed to be utilized within a `[]`.\n",
    "\n",
    "But from my Google searches, it doesn't seem like `[.*]` is a valid regex pattern. I tried using `[\\S\\s]*` but this captured everything, like *literally everything* so I was out of luck there. But why do I care so much about the `\\s`? Well, here is the reason:\n",
    "\n",
    "<div>\n",
    "<img src='img/multiline_bio_html_sp23.png'>\n",
    "</div>\n",
    "\n",
    "For some reason, some bios were not contained in *one line*. There is a newline character in this bio, which required me to somehow capture it in my regex pattern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Scraping of Spring 2023 CS61A AIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those aforementioned edge cases, I start by trying my hand at Spring 2023's CS61A AIs. I implement a few Python functions that performs the extraction and print out checks on the extracted list.\n",
    "\n",
    "These were the initial patterns that I used:\n",
    "* names: `([a-zA-Z\\-\\(\\) ]+)`\n",
    "* pronouns: `'<div class=\"badge badge-info\">([a-zA-Z]*\\/*[a-zA-Z]*\\/*[a-zA-Z]*\\/*)<\\/div>'`\n",
    "* bio: `'<li class=\"section bio\">(.{0,500}\\s?.{0,500}\\s?.{0,500}\\s?.{0,500}\\s?)<\\/li>'`\n",
    "\n",
    "Yes, all of them are absolutely horrid, but I do improve upon them after experiencing more edge cases in other semesters. The \"final\" patterns I used are:\n",
    "* names and pronouns: `'<h3 class=\"staff-name\">[\\s\\n]*([a-zA-Z\\u00C0-\\u00ff\\-\\(\\) ]+)\\s+<div class=\"badge badge-info\">([a-zA-Z\\/ ]*)<\\/div>'`\n",
    "    * accounts for special characters, hyphens, multi-spaces\n",
    "    * combined matching pattern with pronouns, simply because I wanted to\n",
    "    * pronoun matcher accounts for some *very weird* edge cases (listed in later sections)\n",
    "    * some non-generalized patterns were inevitable\n",
    "* bio: `'<li class=\"section bio\">([\\S\\s]{0,500})<\\/li>'`\n",
    "    * mostly stayed the same, which became a constant thorn while I was doing this project, led to the use of some non-generalized patterns\n",
    "\n",
    "Some non-generalized patterns used in this project:\n",
    "* `su22_bio_pattern = '<li class=\"section bio\">([\\S\\s]{0,350})<\\/li>'`\n",
    "    * original pattern was overcapturing, this pattern reduces the character limit\n",
    "* `sp21_bio_pattern = '<li class=\"section bio\">([\\S\\s]{0,400})<\\/li>'`\n",
    "    * same overcapturing issue, character limit reduced\n",
    "* `sp21_name_pronoun_pattern = '<h3>[\\s\\n]*([a-zA-Z\\-\\(\\) ]+)[\\s\\n]*\\(([a-zA-Z\\/ ]*)\\)'`\n",
    "    * inevitable, the `html` file for this semester had a different format from that of other semesters, so instead of creating a mess up a regex pattern to try to generalize for this semester as well, decided to just write another pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEPRECATED CODE ###\n",
    "# This was code that I intially wrote for extracting names and pronouns separately. At the end\n",
    "# I decided to just combine the two patterns, looks neater in my opinion so I kept it.\n",
    "# Functionality wise I don't think this changes much; intead of accessing two arrays containing singular \n",
    "# elements I would just be using a single array containing a list with two elements.\n",
    "\n",
    "# # Extract name\n",
    "# name_pattern = '<h3 class=\"staff-name\">[\\s\\n]*([a-zA-Z ]+)\\n'\n",
    "# name_list = re.findall(name_pattern, response_string)\n",
    "# print(name_list)\n",
    "\n",
    "\n",
    "# # Extract pronouns\n",
    "# pronoun_pattern = '<div class=\"badge badge-info\">([a-zA-Z]*\\/*[a-zA-Z]*\\/*[a-zA-Z]*\\/*)<\\/div>'\n",
    "# pronoun_list = re.findall(pronoun_pattern, response_string)\n",
    "# print(pronoun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish patterns from Inspection section\n",
    "name_pronoun_pattern = '<h3 class=\"staff-name\">[\\s\\n]*([a-zA-Z\\u00C0-\\u00ff\\-\\(\\) ]+)\\s+<div class=\"badge badge-info\">([a-zA-Z\\/ ]*)<\\/div>'\n",
    "bio_pattern = '<li class=\"section bio\">([\\S\\s]{0,500})<\\/li>'\n",
    "\n",
    "# Function that takes in a regex pattern `pattern`, an input String `input_string`, and a boolean `print_info` that\n",
    "# determines whether or not to execute a print statement for debugging purposes.\n",
    "# Will display the total number of elements in the extracted list and return thereof.\n",
    "def extract_pattern(pattern, input_string, print_info=False):\n",
    "    extracted_list = re.findall(pattern, input_string)\n",
    "    if (print_info):\n",
    "        print(\"Length of extracted list: {}\".format(len(extracted_list)))\n",
    "    return extracted_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With my extraction function implemented, I call it below to extract my list of names, pronouns, and bios from Spring 2023 semester's 61A AIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp23_name_pronoun_list:\n",
      "Length of extracted list: 130\n",
      "sp23_bio_list:\n",
      "Length of extracted list: 130\n"
     ]
    }
   ],
   "source": [
    "print('sp23_name_pronoun_list:')\n",
    "sp23_name_pronoun_list = extract_pattern(name_pronoun_pattern, sp23_response_string, True)\n",
    "\n",
    "print('sp23_bio_list:')\n",
    "sp23_bio_list = extract_pattern(bio_pattern, sp23_response_string, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be working as intended, $130/130$ matches. I'm sure I'll run into more issues for other semesters, so I'll take care of it then."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the AIs of Other Semesters of 61A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having been in the AI program for three semesters, I'm aware that AIs aren't always hired every semester, and it was only \"rebranded\" recently. When I first started university, I think they were just called lab assistants; essentially unpaid TAs that Berkeley needed since there wasn't enough teaching staff, but this is a whole another discussion that I don't really get into.\n",
    "\n",
    "Knowing this, I took a closer look at the AI pages for each recent semester of 61A (as of 4/15/2023), and these were my findings:\n",
    "\n",
    "CS61A AI Program:\n",
    "* Fall Semesters\n",
    "    * [FA22](https://inst.eecs.berkeley.edu/~cs61a/fa22/academic-interns/) - exists\n",
    "    * [FA21](https://inst.eecs.berkeley.edu/~cs61a/fa21/academic-interns/) - exists\n",
    "    * [FA20](https://inst.eecs.berkeley.edu/~cs61a/fa20/) - does not exist\n",
    "* Spring Semesters\n",
    "    * [SP23](https://inst.eecs.berkeley.edu/~cs61a/sp23/academic-interns/) - exists (scraped it above)\n",
    "    * [SP22](https://inst.eecs.berkeley.edu/~cs61a/sp22/academic-interns/) - exists\n",
    "    * [SP21](https://inst.eecs.berkeley.edu/~cs61a/sp21/academic-interns/) - exists BUT pronouns seem to be joined together with name, need a different regex pattern\n",
    "    * [SP20](https://inst.eecs.berkeley.edu/~cs61a/sp20/academic-interns.html) - exists BUT no pronouns\n",
    "* Summer Semesters\n",
    "    * [SU22](https://inst.eecs.berkeley.edu/~cs61a/su22/academic-interns/) - exists\n",
    "    * [SU21](https://inst.eecs.berkeley.edu/~cs61a/su21/academic-interns/) - *page* exists BUT actual AIs do not\n",
    "\n",
    "With these observartions, I'm going to scrape the pages that contain AIs first, then I'll look closer into the semesters containing special cases and see what to do from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping FA22 AIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total AIs:** $152$\n",
    "\n",
    "In running my function on the [CS61A FA22 AI](https://inst.eecs.berkeley.edu/~cs61a/fa22/academic-interns/) page, I ran into some more of the same edge cases:\n",
    "\n",
    "* Multi-set pronoun, just like SP23, I accept the first set of pronouns\n",
    "<div>\n",
    "<img src='img/multi_pronoun_html_fa22.png' width=500>\n",
    "</div>\n",
    "\n",
    "\n",
    "* Multiline bio, this one had a lot more newlines compared to that of SP23, forced me to change my bio extraction pattern\n",
    "<div>\n",
    "<img src='img/multiline_bio_html_fa22.png' width=750>\n",
    "</div>\n",
    "\n",
    "* But the *most egregious* of all...\n",
    "<div>\n",
    "<img src='img/comrade_pronoun.png' width=250>\n",
    "</div>\n",
    "\n",
    "Nothing against communism but c'mon Winston, you're making my life so much harder.\n",
    "\n",
    "With all these edge cases, I update my patterns `name_pronoun_pattern` and `bio_pattern`. Then I perform the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of extracted list: 152\n",
      "Length of extracted list: 152\n"
     ]
    }
   ],
   "source": [
    "# API call to get the `html` file\n",
    "fa22_response_string = requests.get('https://inst.eecs.berkeley.edu/~cs61a/fa22/academic-interns/').text\n",
    "\n",
    "# Extract the name and pronoun list\n",
    "fa22_name_pronoun_list = extract_pattern(name_pronoun_pattern, fa22_response_string, True)\n",
    "# Extract the bio list\n",
    "fa22_bio_list = extract_pattern(bio_pattern, fa22_response_string, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$152/152$ FA22 AIs extracted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping SP22 AIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total AIs:** $125$\n",
    "\n",
    "Moving on to the [CS61A SP22 AI](https://inst.eecs.berkeley.edu/~cs61a/sp22/academic-interns/) page, I ran into one edge case:\n",
    "\n",
    "<div>\n",
    "<img src='img/special_character_name.png', width=500>\n",
    "</div>\n",
    "\n",
    "To capture the special character, I referred to this [Stack Overflow post](https://stackoverflow.com/questions/2013451/test-if-string-contains-only-letters-a-z-%C3%A9-%C3%BC-%C3%B6-%C3%AA-%C3%A5-%C3%B8-etc) and adjusted by `name_pronoun_pattern` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of extracted list: 125\n",
      "Length of extracted list: 125\n"
     ]
    }
   ],
   "source": [
    "# API call to get the `html` file\n",
    "sp22_response_string = requests.get('https://inst.eecs.berkeley.edu/~cs61a/sp22/academic-interns/').text\n",
    "\n",
    "# Extract the name and pronoun list\n",
    "sp22_name_pronoun_list = extract_pattern(name_pronoun_pattern, sp22_response_string, True)\n",
    "# Extract the bio list\n",
    "sp22_bio_list = extract_pattern(bio_pattern, sp22_response_string, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes I am aware that I am copy pasting code. I did consider capturing the extraction process within a function, but I think it would be more trouble than its worth, since I want to be able to assign my extracted lists to a named variable. I guess I could create a function that returns a tuple of the two lists, but then that would just require more work from me to retrieve those lists and write them into a file. So this time, I'll sacrifice aesthetics for ease of use. $125/125$ extracted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping SU22 AIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total AIs:** $54$\n",
    "\n",
    "Moving on to the [CS61A SU22 AI](https://inst.eecs.berkeley.edu/~cs61a/su22/academic-interns/) page:\n",
    "\n",
    "There was an edge case exclusive to the `html` file of these AI which resulted in my `bio_pattern` capturing too much:\n",
    "\n",
    "<div>\n",
    "<img src='img/bio_cap_length_su22.png' width=750>\n",
    "</div>\n",
    "\n",
    "I could change `bio_pattern` again, but I've already modified it inefficiently so many times I think I'm just going to create an exclusive bio pattern, `su22_bio_pattern` that captures less to take care of this edge case. Not the most beautiful solution but, hey, it works.\n",
    "\n",
    "But the edge case that gave me the biggest headache was this:\n",
    "\n",
    "<div>\n",
    "<img src='img/name_hyperlink_su22.png', width=1000>\n",
    "</div>\n",
    "\n",
    "Someone *hyperlinked* their name...\n",
    "\n",
    "I really don't want to complicated my current *working* regex pattern any further, so to take care of this problem, I use [Regex101](https://regex101.com/) to determine the position of the capture. Matches start enumerating from $1$ and according to Regex101, this edge caes should be match $31$, which means it should sit in array index position $30$. Thus, I will copy the contents of the extracted list from index $0$ to $29$ (thirty names), append this ~~*cursed*~~ special name to index $30$, then append the remaining tail of the extracted list, specifically the original elements in index $30$ to $52$ (twenty-three names), to the end of my new list. This should result in $30 + 1 + 23 = 54$ names in my final list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of extracted list: 53\n",
      "Length of extracted list: 54\n"
     ]
    }
   ],
   "source": [
    "# API call to get the `html` file\n",
    "su22_response_string = requests.get('https://inst.eecs.berkeley.edu/~cs61a/su22/academic-interns/').text\n",
    "\n",
    "# Extract the name and pronoun list to create a \"pre\" list that needs to be modified later\n",
    "su22_name_pronoun_pre_list = extract_pattern(name_pronoun_pattern, su22_response_string, True)\n",
    "# Extract the bio list\n",
    "su22_bio_pattern = '<li class=\"section bio\">([\\S\\s]{0,350})<\\/li>'\n",
    "su22_bio_list = extract_pattern(su22_bio_pattern, su22_response_string, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to creating the *correct* `su22_name_pronoun_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of modified list: 54\n"
     ]
    }
   ],
   "source": [
    "# Get the first thirty elements\n",
    "su22_name_pronoun_list = su22_name_pronoun_pre_list[0:30]\n",
    "# Use list.append() to add the special name, pronoun pair: (Matthew Lee, he/him/his)\n",
    "su22_name_pronoun_list.append(('Matthew Lee', 'he/him/his'))\n",
    "# Use list.extend() to add the remaining elements in the tail of the orginal extracted list\n",
    "su22_name_pronoun_list.extend(su22_name_pronoun_pre_list[30:])\n",
    "# Check length of the final list for accuracy\n",
    "print(\"Length of modified list: {}\".format(len(su22_name_pronoun_list)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess that wasn't too bad. $53/54$ extracted with $1$ imputation. Moving on to other semesters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping FA21 AIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Total AIs:** $132$\n",
    "\n",
    "Moving on to the [CS61A FA21 AI](https://inst.eecs.berkeley.edu/~cs61a/fa21/academic-interns/) page, I encountered a very interesting edge case:\n",
    "\n",
    "<div>\n",
    "<img src='img/missing_pronoun_html_fa21.png', width=500>\n",
    "<img src='img/missing_pronoun_html_2_fa21.png', width=575>\n",
    "</div>\n",
    "\n",
    "There were *two* entries where the pronouns section *does not* show up in the `html` file. Usually if a student did not list their pronouns, the section would still exist in the `html` file, but it would just be empty.\n",
    "\n",
    "To take care of this issue, I decided to go the manual input route, the same way I solved the ~~*cursed*~~ hyperlinked name issue. With that said, using [Regex101](https://regex101.com/) again helped me determine their supposed positions. From looking at their names and photos provided on the page, I'm also going to commit the 2023 sin of assuming their genders. I've determined that \"Irene Geng, she/her/hers\" will be in array index position $55$ and \"Wonjae Lee, he/him/his\" will be in array index position $126$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of extracted list: 130\n",
      "Length of extracted list: 132\n"
     ]
    }
   ],
   "source": [
    "# API call to get the `html` file\n",
    "fa21_response_string = requests.get('https://inst.eecs.berkeley.edu/~cs61a/fa21/academic-interns/').text\n",
    "\n",
    "# Extract the name and pronoun list\n",
    "fa21_name_pronoun_pre_list = extract_pattern(name_pronoun_pattern, fa21_response_string, True)\n",
    "# Extract the bio list\n",
    "fa21_bio_list = extract_pattern(bio_pattern, fa21_response_string, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the list manual inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of modified list: 132\n",
      "Tuple at index 55: ('Irene Geng', 'she/her/hers')\n",
      "Tuple at index 126: ('Wonjae Lee', 'he/him/his')\n"
     ]
    }
   ],
   "source": [
    "# Get the first fifty-five elements\n",
    "fa21_name_pronoun_list = fa21_name_pronoun_pre_list[0:55]\n",
    "# Use list.append() to add the first name, pronoun pair: (Irene Geng, she/her/hers)\n",
    "fa21_name_pronoun_list.append(('Irene Geng', 'she/her/hers'))\n",
    "# Use list.extend() to add the middle elements the orginal extracted list\n",
    "fa21_name_pronoun_list.extend(fa21_name_pronoun_pre_list[55:125])\n",
    "# Use list.append() to add the second name, pronoun pair: (Wonjae Lee, he/him/his)\n",
    "fa21_name_pronoun_list.append(('Wonjae Lee', 'he/him/his'))\n",
    "# Use list.extend() to add the remaining tail of the original extracted list\n",
    "fa21_name_pronoun_list.extend(fa21_name_pronoun_pre_list[125:])\n",
    "\n",
    "# Printing some checks\n",
    "# Check length of the final list for accuracy\n",
    "print(\"Length of modified list: {}\".format(len(fa21_name_pronoun_list)))\n",
    "# Check entries at position 55 and 126\n",
    "print(\"Tuple at index 55: {}\".format(str(fa21_name_pronoun_list[55])))\n",
    "print(\"Tuple at index 126: {}\".format(str(fa21_name_pronoun_list[126])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it worked properly. $130/132$ extracted with $2$ imputations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping SP21 AIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total AIs:** $50$\n",
    "\n",
    "Moving on to the [CS61A SP21 AI ](https://inst.eecs.berkeley.edu/~cs61a/sp21/academic-interns/) page.\n",
    "\n",
    "Additionally, the `html` file of this page is different from that of the others:\n",
    "\n",
    "<div>\n",
    "<img src='img/html_file_sp21.png'>\n",
    "</div>\n",
    "\n",
    "This meant that I need to change my patterns a little bit. From experimenting around, I ran into a similar issue as SU22 for my `bio_pattern`. Thus, I ended up changing this semester's pattern to: `<li class=\"section bio\">([\\S\\s]{0,400})<\\/li>`\n",
    "\n",
    "Furthermore, the pattern to capture name and pronouns also needed to be changed. I settled on the following pattern: `<h3>[\\s\\n]*([a-zA-Z\\-\\(\\) ]+)[\\s\\n]*\\(([a-zA-Z\\/ ]*)\\)`\n",
    "\n",
    "Nevertheless, the pattern isn't perfect due to encountering the same hyperlink name edge cases as SU22:\n",
    "\n",
    "<div>\n",
    "<img src='img/name_hyperlink_sp21.png', width=450>\n",
    "<img src='img/name_hyperlink_2_sp21.png', width=500>\n",
    "</div>\n",
    "\n",
    "Since I'm already in so deep, I will be manually inputting these two values. Again, using [Regex101](https://regex101.com/), I determined that they belong in index postions $16$ and $43$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of extracted list: 48\n",
      "Length of extracted list: 50\n"
     ]
    }
   ],
   "source": [
    "# API call to get the `html` file\n",
    "sp21_response_string = requests.get('https://inst.eecs.berkeley.edu/~cs61a/sp21/academic-interns/').text\n",
    "\n",
    "# Extract the name and pronoun list\n",
    "sp21_name_pronoun_pattern = '<h3>[\\s\\n]*([a-zA-Z\\-\\(\\) ]+)[\\s\\n]*\\(([a-zA-Z\\/ ]*)\\)'\n",
    "sp21_name_pronoun_pre_list = extract_pattern(sp21_name_pronoun_pattern, sp21_response_string, True)\n",
    "# Extract the bio list\n",
    "sp21_bio_pattern = '<li class=\"section bio\">([\\S\\s]{0,400})<\\/li>'\n",
    "sp21_bio_list = extract_pattern(sp21_bio_pattern, sp21_response_string, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same procedure as FA21, performing the list manual inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of modified list: 50\n",
      "Tuple at index 16: ('Devin Sze', 'he/him/his')\n",
      "Tuple at index 43: ('Wonjae Lee', 'he/him/his')\n"
     ]
    }
   ],
   "source": [
    "# Get the first sixteen elements\n",
    "sp21_name_pronoun_list = sp21_name_pronoun_pre_list[0:16]\n",
    "# Use list.append() to add the first name, pronoun pair: (Irene Geng, she/her/hers)\n",
    "sp21_name_pronoun_list.append(('Devin Sze', 'he/him/his'))\n",
    "# Use list.extend() to add the middle elements the orginal extracted list\n",
    "sp21_name_pronoun_list.extend(sp21_name_pronoun_pre_list[16:42])\n",
    "# Use list.append() to add the second name, pronoun pair: (Wonjae Lee, he/him/his)\n",
    "sp21_name_pronoun_list.append(('Wonjae Lee', 'he/him/his'))\n",
    "# Use list.extend() to add the remaining tail of the original extracted list\n",
    "sp21_name_pronoun_list.extend(sp21_name_pronoun_pre_list[42:])\n",
    "\n",
    "# Printing some checks\n",
    "# Check length of the final list for accuracy\n",
    "print(\"Length of modified list: {}\".format(len(sp21_name_pronoun_list)))\n",
    "# Check entries at position 55 and 126\n",
    "print(\"Tuple at index 16: {}\".format(str(sp21_name_pronoun_list[16])))\n",
    "print(\"Tuple at index 43: {}\".format(str(sp21_name_pronoun_list[43])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. $48/52$ extracted with $2$ imputations. And with that, we are done with scraping all the data that I will be using.\n",
    "\n",
    "Next is writing it into a `.csv` file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Extracted Data into a `.csv` File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally. With all the data successfully scraped and the missing data imputed (done with some personal liberties), I am ready to write the data into a `.csv` file. For my `.csv` file, I plan on formatting it with the following headers:\n",
    "\n",
    "`name`, `pronoun`, `bio`, `semester`\n",
    "\n",
    "Below, I implement a Python function to do just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function that writes all the data scraped into a .csv file. Since the data has already been extracted\n",
    "# in the earlier parts of the notebook, I plan on using those list variables instead of encapsulating the capture\n",
    "# within this function. This means less work for me, but it also means the inevitable use of the match-case block\n",
    "# below, which probably isn't the most aesthetically pleasing.\n",
    "# Nevertheless, function takes in nothing but writes in the data I desire into a .csv file named `ai_61a.data.csv`.\n",
    "def write_data():\n",
    "    # open a destination file to write into\n",
    "    ai_61a_data = open(\"ai_61a_data.csv\", \"w\")\n",
    "    # write the headers\n",
    "    ai_61a_data.write(\"name,pronoun,bio,semester\\n\")\n",
    "\n",
    "    # instantiate a list of semesters to loop through\n",
    "    sem_list = ['fa22', 'sp22', 'su22', 'fa21', 'sp21']\n",
    "    # loop through the list\n",
    "    for sem in sem_list:\n",
    "\n",
    "        # Initialize empty lists\n",
    "        curr_name_pronoun_list = []\n",
    "        curr_bio_list = []\n",
    "\n",
    "        # Decided to use a match-case block here, probably not the most elegant solution\n",
    "        # Needed to determine which semester's list to start working on\n",
    "        match sem:\n",
    "            case 'fa22':\n",
    "                curr_name_pronoun_list = fa22_name_pronoun_list\n",
    "                curr_bio_list = fa22_bio_list\n",
    "            case 'sp22':\n",
    "                curr_name_pronoun_list = sp22_name_pronoun_list\n",
    "                curr_bio_list = sp22_bio_list\n",
    "            case 'su22':\n",
    "                curr_name_pronoun_list = su22_name_pronoun_list\n",
    "                curr_bio_list = su22_bio_list\n",
    "            case 'fa21':\n",
    "                curr_name_pronoun_list = fa21_name_pronoun_list\n",
    "                curr_bio_list = fa21_bio_list\n",
    "            case 'sp21':\n",
    "                curr_name_pronoun_list = sp21_name_pronoun_list\n",
    "                curr_bio_list = sp21_bio_list\n",
    "        \n",
    "        # Zip and loop through the two extracted lists\n",
    "        for name_pronoun_tuple, bio_element in zip(curr_name_pronoun_list, curr_bio_list):\n",
    "\n",
    "            # instantiate variables for each feature\n",
    "            name = name_pronoun_tuple[0] # name is the 0th index of the tuple\n",
    "            pronoun = name_pronoun_tuple[1] # pronoun is the 1st index of the tuple\n",
    "            bio = bio_element\n",
    "\n",
    "            ### SOME CLEANING NEEDED. NEED TO GET RID OF ALL COMMAS AND NEWLINES IN BIO ###\n",
    "            bio = re.sub(',', '', bio)\n",
    "            bio = re.sub('\\n', '', bio)\n",
    "            \n",
    "            # concatenate all features together to form tuple\n",
    "            curr_tuple = name + ',' + pronoun + ',' + bio + ',' + sem + '\\n'\n",
    "            # write tuple into destination file\n",
    "            ai_61a_data.write(curr_tuple)\n",
    "\n",
    "    ai_61a_data.close() # close the destination file\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `write_data` successfully implemented, I call the function in the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data\n",
    "write_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It ran instantly! With the data scraping done, I move on to EDA and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
